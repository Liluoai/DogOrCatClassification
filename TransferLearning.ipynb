{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导出特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "from keras.applications.resnet50 import ResNet50, decode_predictions\n",
    "from keras.applications import *\n",
    "import cv2, os, itertools\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'data/train/'\n",
    "TEST_DIR = 'data/test/'\n",
    "\n",
    "train_dir = [TRAIN_DIR + i for i in os.listdir(TRAIN_DIR)]\n",
    "# train_dog_dir = [TRAIN_DIR + i for i in os.listdir(TRAIN_DIR) if 'dog' in i]\n",
    "# train_cat_dir = [TRAIN_DIR + i for i in os.listdir(TRAIN_DIR) if 'cat' in i]\n",
    "test_dir = [TEST_DIR + i for i in os.listdir(TEST_DIR)]\n",
    "\n",
    "# train_dir = train_dir[:50]\n",
    "# test_dir = test_dir[:50]\n",
    "train_dir.sort()\n",
    "test_dir.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save feature vector to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concat_train_feature_vector = np.ndarray((len(train_dir), 2048 + 2048 + 512 + 512), dtype = np.float32)\n",
    "concat_test_feature_vector = np.ndarray((len(test_dir), 2048 + 2048 + 512 + 512), dtype = np.float32)\n",
    "\n",
    "def get_feature_vector_list(image_path_list, MODEL, input_size, preprocess_fun = None):\n",
    "    inputs = Input(input_size)\n",
    "    #注意: 预处理函数对于Xception和InceptionV3都是必须的, 否则会预测出错\n",
    "    if preprocess_fun:\n",
    "        inputs = Lambda(preprocess_fun)(inputs)\n",
    "        \n",
    "    #❓我的组合模型为什么错了\n",
    "#     x = MODEL(input_tensor = inputs, include_top = False).output\n",
    "#     outputs = GlobalAveragePooling2D()(x)\n",
    "#     model = Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "    #别人的正确模型\n",
    "    base_model = MODEL(input_tensor = inputs, weights = 'imagenet', include_top = False)\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "\n",
    "    #不修改的模型\n",
    "#     model = MODEL(input_tensor = inputs, weights='imagenet')\n",
    "\n",
    "    print('start {} prediction:'.format(base_model.name))\n",
    "    feature_vector_list = []\n",
    "    for i, image_path in enumerate(image_path_list):\n",
    "        input_image = prepare_data(image_path, input_size)\n",
    "        input_image = np.expand_dims(input_image, axis = 0)\n",
    "        feature_vector = model.predict(input_image, verbose = 0)\n",
    "        feature_vector_list.append(feature_vector)\n",
    "        #显示进度条\n",
    "        if i%(len(image_path_list)//100) == 0:\n",
    "            print('>', end = '')\n",
    "    print('finish {} prediction'.format(base_model.name))\n",
    "    \n",
    "    return feature_vector_list\n",
    "\n",
    "        \n",
    "def prepare_data(image_path, input_size):\n",
    "    rows = input_size[0]\n",
    "    cols = input_size[1]\n",
    "    channels = input_size[2]\n",
    "    data = np.ndarray(input_size, dtype = np.uint8)\n",
    "    \n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (rows, cols), interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def get_and_save_feature_vector():\n",
    "    #ResNet50输出(1, 2048)\n",
    "    ResNet50_train_feature_vector_list = get_feature_vector_list(train_dir, ResNet50, (224, 224, 3))\n",
    "    #Xception输出(1, 2048)\n",
    "    Xception_train_feature_vector_list = get_feature_vector_list(train_dir, Xception, (299, 299, 3), xception.preprocess_input)\n",
    "    #TODO: 修正InceptionV3模型的错误使用\n",
    "    # get_feature_vector(image_path, InceptionV3, (299, 299, 3), inception_v3.preprocess_input)\n",
    "    #VGG16输出(1, 512)\n",
    "    VGG16_train_feature_vector_list = get_feature_vector_list(train_dir, VGG16, (224, 224, 3))\n",
    "    #VGG19输出(1, 512)\n",
    "    VGG19_train_feature_vector_list = get_feature_vector_list(train_dir, VGG19, (224, 224, 3))\n",
    "    for i in range(len(train_dir)):\n",
    "        concat_train_feature_vector[i] = np.concatenate([ResNet50_train_feature_vector_list[i], \n",
    "                                                        Xception_train_feature_vector_list[i],\n",
    "                                                        VGG16_train_feature_vector_list[i],\n",
    "                                                        VGG19_train_feature_vector_list[i]], axis=1)\n",
    "    \n",
    "    ResNet50_test_feature_vector_list = get_feature_vector_list(test_dir, ResNet50, (224, 224, 3))\n",
    "    Xception_test_feature_vector_list = get_feature_vector_list(test_dir, Xception, (299, 299, 3), xception.preprocess_input)\n",
    "    VGG16_test_feature_vector_list = get_feature_vector_list(test_dir, VGG16, (224, 224, 3))\n",
    "    VGG19_test_feature_vector_list = get_feature_vector_list(test_dir, VGG19, (224, 224, 3))\n",
    "    for i in range(len(test_dir)):\n",
    "        concat_test_feature_vector[i] = np.concatenate([ResNet50_test_feature_vector_list[i], \n",
    "                                                        Xception_test_feature_vector_list[i],\n",
    "                                                        VGG16_test_feature_vector_list[i],\n",
    "                                                        VGG19_test_feature_vector_list[i]], axis=1)\n",
    "    \n",
    "    np.savetxt(\"concat_train_feature_vector.npy\", concat_train_feature_vector, delimiter = ',')\n",
    "    np.savetxt(\"concat_test_feature_vector.npy\", concat_test_feature_vector, delimiter = ',')\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "get_and_save_feature_vector() \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read feature vector from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_test_feature_vector = np.loadtxt(open(\"concat_test_feature_vector.npy\",\"rb\"), delimiter=\",\").astype(np.float32)\n",
    "concat_train_feature_vector = np.loadtxt(open(\"concat_train_feature_vector.npy\",\"rb\"), delimiter=\",\").astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5120)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_train_feature_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "labels = []\n",
    "for train_image_dir in train_dir:\n",
    "    if 'dog' in train_image_dir:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "        \n",
    "y_train = np.array(labels)\n",
    "x_train, y_train = shuffle(concat_train_feature_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define new model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dog-project/lib/python3.5/site-packages/keras/engine/training.py:1393: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/8\n",
      "20000/20000 [==============================] - 1s - loss: 0.3707 - acc: 0.8913 - val_loss: 0.0573 - val_acc: 0.9778\n",
      "Epoch 2/8\n",
      "20000/20000 [==============================] - 0s - loss: 0.0895 - acc: 0.9707 - val_loss: 0.0458 - val_acc: 0.9840\n",
      "Epoch 3/8\n",
      "20000/20000 [==============================] - 0s - loss: 0.0602 - acc: 0.9805 - val_loss: 0.0395 - val_acc: 0.9854\n",
      "Epoch 4/8\n",
      "20000/20000 [==============================] - 0s - loss: 0.0470 - acc: 0.9845 - val_loss: 0.0340 - val_acc: 0.9882\n",
      "Epoch 5/8\n",
      "20000/20000 [==============================] - 0s - loss: 0.0391 - acc: 0.9863 - val_loss: 0.0349 - val_acc: 0.9880\n",
      "Epoch 6/8\n",
      "20000/20000 [==============================] - 0s - loss: 0.0323 - acc: 0.9884 - val_loss: 0.0323 - val_acc: 0.9892\n",
      "Epoch 7/8\n",
      "20000/20000 [==============================] - 0s - loss: 0.0309 - acc: 0.9903 - val_loss: 0.0302 - val_acc: 0.9900\n",
      "Epoch 8/8\n",
      "20000/20000 [==============================] - 0s - loss: 0.0285 - acc: 0.9909 - val_loss: 0.0301 - val_acc: 0.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00],\n",
       "       [  8.01979768e-05],\n",
       "       [  1.04454593e-05],\n",
       "       ..., \n",
       "       [  1.00000000e+00],\n",
       "       [  9.30272051e-07],\n",
       "       [  2.91424931e-06]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape = (x_train.shape[1], ))\n",
    "x = Dropout(0.5)(inputs)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs = inputs, outputs = predictions)\n",
    "model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, nb_epoch=8, validation_split=0.2)\n",
    "y_pre = model.predict(concat_test_feature_vector)\n",
    "y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = y_pre.clip(min=0.005, max=0.995)\n",
    "import csv\n",
    "\n",
    "def sort_y_pre_by_id(y_pre, test_dir):\n",
    "    y_pre_order_by_id = np.zeros((len(y_pre),))\n",
    "    for i in range(len(test_dir)):\n",
    "        idx = int(test_dir[i][10:-4]) - 1\n",
    "#         print([i, idx])\n",
    "        y_pre_order_by_id[idx] = y_pre[i]\n",
    "    return y_pre_order_by_id\n",
    "\n",
    "y_pre_order_by_id = sort_y_pre_by_id(y_pre, test_dir)\n",
    "\n",
    "#python2可以用file替代open\n",
    "with open(\"submission.csv\",\"w\") as csvfile: \n",
    "    writer = csv.writer(csvfile)\n",
    "    #先写入columns_nameΩΩΩ\n",
    "    writer.writerow([\"id\",\"label\"])\n",
    "    for i in range(len(y_pre_order_by_id)):\n",
    "        writer.writerow([i + 1, y_pre_order_by_id[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog-project",
   "language": "python",
   "name": "dog-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
